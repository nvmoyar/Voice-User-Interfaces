{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition (ASR)\n",
    "\n",
    "## Challenges in ASR\n",
    "\n",
    "1. Background noise\n",
    "2. Variability of the speaker (pitch, volume)\n",
    "3. Same word, different speeds\n",
    "4. Word Boundaries\n",
    "5. Spoken language vs written language \n",
    "\n",
    "\n",
    "## Pipeline for ASR\n",
    "\n",
    "<img src=\"assets/asr-pipeline.png\" />\n",
    "\n",
    "So far, with MFCC we can extract features from speech and we can address challenges 1 and 2. This features can be turned on phonetic representation or phonemes using an acoustic model. Phonemes can be translated into words using a Lexical Decoding or Lexicon. However there are systems capable to translate the acoustic model into words. This is a **design choice** indeed, and it depends on the dimensionality of the problem. \n",
    "\n",
    "\n",
    "## The acoustic model and trouble with time: same word, different speeds --> HMM's\n",
    "\n",
    "The problem we address here, is the common fact that the words are usually pronounced using a diferent time length. For instance, besides the different pronunciations in the word 'Hello\", it doesn't usually take the same time saying it. Further, some words might sound the same and only from the acoustic model, without providing any detail about the context, are really hard to distinguish like HEAR or HERE. \n",
    "\n",
    "Hidden Markov Models (HMM) are specially good to address the problem of variability in length, since they are really good to find patterns through time. The training set could consists of all labelled words, phonemes, sounds or groups of words, in order to determine the likelihood of a single word or phoneme. However this training becomes much more complicated when our dataset consists of utterances -phrases or whole sentences-. How could we this series of data be separated in training? Since a particular word may be connected to the previous and the next words, in order to train continuous utterances, HMM nodes are tied together as pairs which leads to an increase of dimensionality. When training words, the combinations are unbeareable, however training phonemes is a more accessible problem, since there are only 40 phonemes in English, only 1600 combinations are possible. Once the model is trained, it can be used to score new utterances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding knowledge: Language model\n",
    "\n",
    "Which combinations of words are more reasonable? Even if from phonemes we can get words, we still can't solve language ambiguities in spelling and context, since we haven't taught to the model which combinations are more likely, or at least provide knowledge about the context, to allow the model to learn from itself. The language models just does this job: inject knowledge. \n",
    "\n",
    "Every single word can be thought as a probability of distribution over many different words. Each possible sequence can be calculated as the likelihood that a particular word could have been produced by the audio signal.\n",
    "\n",
    "$$P(signal   |   w_1,w_2)$$\n",
    "\n",
    "A statistical language model does precisely that. It provides a probability distribution over sequences of words. \n",
    "\n",
    "$$word_1, word_2, ... = argmax_{w_1 w_2 ...} {P(signal | w_1,w_2,...) * P(w_1,w_2,...)}$$\n",
    "\n",
    "Even though, the dimensionality applying a statistical model is extremely huge, and some heuristics or approximate can be employed here: **It turns out that in practice, the words we speak at any time are primarily dependent upon the three to four previous words.**\n",
    "\n",
    "### N-grams\n",
    "\n",
    "N-grams are prob of single words \"I\", ordered pairs \"I love\" (bigrams), triples \"I love Science\", etc. With n-grams we can approximate the sequence probability using the chaing rule. \n",
    "\n",
    "$$ P(\"I\", \"love\", \"Science\") = P(\"I\") * P(\"love\"|\"I\") * P(\"Science\" | \"I\", \"love\") $$\n",
    "\n",
    "Then we can score these probability along with the probabilities coming from the Acoustic model to remove language ambiguities from the sequence options and **provide a better estimate of the utterance give an text**. \n",
    "\n",
    "#### Quizz: computing bigrams\n",
    "\n",
    "In the following series of quizes, you will work with 2-grams, or bigrams, as they are more commonly called. The objective is to create a function that calculates the probability that a particular sentence could occur in a corpus of text, based on the probabilities of its component bigrams. We'll do this in stages though:\n",
    "\n",
    "* Quiz 1 - Extract tokens and bigrams from a sentence\n",
    "* Quiz 2 - Calculate probabilities for bigrams\n",
    "* Quiz 3 - Calculate the log probability of a given sentence based on a corpus of text using bigrams\n",
    "\n",
    "##### Assumptions and terminology\n",
    "\n",
    "* Utterance : 'I love language models'\n",
    "* Tokens (word list from utterance + start tag + ending tag) : ['<s>', 'i', 'love', 'language', 'models', '</s>']\n",
    "* Bigrams: The bigrams for this sentence are represented as a list of lower case ordered pairs of tokens:\n",
    "\n",
    "bigrams = [('<s>', 'i'), ('i', 'love'), ('love', 'language'), ('language', 'models'), ('models', '</s>')]\n",
    "\n",
    "##### Quiz 1 Instructions\n",
    "\n",
    "In the quiz below, write a function that returns a list of tokens and a list of bigrams for a given sentence. You will need to first break a sentence into words in a list, then add a <s> and <s/> token to the start and end of the list to represent the start and end of the sentence.\n",
    "\n",
    "Your final lists should be in the format shown above and called out in the function doc string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['<s>', 'the', 'old', 'man', 'spoke', 'to', 'me', '</s>'],\n",
       "  [('<s>', 'the'),\n",
       "   ('the', 'old'),\n",
       "   ('old', 'man'),\n",
       "   ('man', 'spoke'),\n",
       "   ('spoke', 'to'),\n",
       "   ('to', 'me'),\n",
       "   ('me', '</s>')]),\n",
       " (['<s>', 'me', 'to', 'spoke', 'man', 'old', 'the', '</s>'],\n",
       "  [('<s>', 'me'),\n",
       "   ('me', 'to'),\n",
       "   ('to', 'spoke'),\n",
       "   ('spoke', 'man'),\n",
       "   ('man', 'old'),\n",
       "   ('old', 'the'),\n",
       "   ('the', '</s>')]),\n",
       " (['<s>', 'old', 'man', 'me', 'old', 'man', 'me', '</s>'],\n",
       "  [('<s>', 'old'),\n",
       "   ('old', 'man'),\n",
       "   ('man', 'me'),\n",
       "   ('me', 'old'),\n",
       "   ('old', 'man'),\n",
       "   ('man', 'me'),\n",
       "   ('me', '</s>')])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'the old man spoke to me',\n",
    "    'me to spoke man old the',\n",
    "    'old man me old man me',\n",
    "]\n",
    "\n",
    "\n",
    "def get_token(sentence): \n",
    "    token_list = sentence.split(\" \")\n",
    "    token_list.insert(0, \"<s>\")\n",
    "    token_list.append(\"</s>\")   \n",
    "    return token_list\n",
    "\n",
    "\n",
    "def get_bigram(token_list): \n",
    "    \n",
    "    pairs=[]  \n",
    "    [pairs.append(token_list[i]) for i in range(1, len(token_list))]       \n",
    "    return list(zip(token_list, pairs))\n",
    "\n",
    "def sentence_to_bigrams(sentence):\n",
    "    \"\"\"\n",
    "    Add start '<s>' and stop '</s>' tags to the sentence and tokenize it into a list\n",
    "    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)\n",
    "    :param sentence: string\n",
    "    :return: list, list\n",
    "        sentence_tokens: ordered list of words found in the sentence\n",
    "        sentence_bigrams: a list of ordered two-word tuples found in the sentence\n",
    "    \"\"\"\n",
    "    sentence_tokens = get_token(sentence)\n",
    "    sentence_bigrams = get_bigram(sentence_tokens)\n",
    "    \n",
    "    return sentence_tokens, sentence_bigrams\n",
    "\n",
    "[sentence_to_bigrams(sentence) for sentence in test_sentences] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probabilities and Likelihoods with Bigrams\n",
    "\n",
    "The probability of a series of words can be calculated from the chained probabilities of its history:\n",
    "\n",
    "$$  P_{w_1, w_2, ...,w_n} = \\prod_{i=1}^{n} P(w_i| w_1 w_2,...,w_{i-1})$$\n",
    "\n",
    "The probabilities of sequence occurrences in a large textual corpus can be calculated this way and used as a **language model to add grammar and contextual knowledge to a speech recognition system**. However, there is a prohibitively large number of calculations for all the possible sequences of varying length in a large textual corpus.\n",
    "\n",
    "To address this problem, we use the Markov Assumption to approximate a sequence probability with a shorter sequence.\n",
    "\n",
    "###### Markov Assumption\n",
    "\n",
    "In probability theory and statistics, the term Markov property refers to the memoryless property of a stochastic process. It is named after the Russian mathematician Andrey Markov.\n",
    "\n",
    "**A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it**. A process with this property is called a Markov process. The term strong Markov property is similar to the Markov property, except that the meaning of \"present\" is defined in terms of a random variable known as a stopping time.\n",
    "\n",
    "The term Markov assumption is used to describe a model where the Markov property is assumed to hold, such as a hidden Markov model.\n",
    "\n",
    "A Markov random field extends this property to two or more dimensions or to random variables defined for an interconnected network of items. An example of a model for such a field is the Ising model. A discrete-time stochastic process satisfying the Markov property is known as a Markov chain.\n",
    "\n",
    "Thus, we could approximate\n",
    "\n",
    "$$  P_{w_1, w_2, ...,w_n} \\approx \\prod_{i=1}^{n} P(w_i| w_{i-k}...w_{i-1})$$\n",
    "\n",
    "\n",
    "We can calculate the probabilities by using counts of the bigrams and individual tokens. The counts are represented below with the c() operator:\n",
    "\n",
    "$$  P_{w_i|w_{i-1}} = \\dfrac{c(w_{i-1}, w_i)}{c(w_{i-1})}$$\n",
    "\n",
    "In the quiz below, write a function that returns a probability dictionary when given a lists of tokens and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigrams_from_transcript(filename):\n",
    "    \"\"\"\n",
    "    read a file of sentences, adding start '<s>' and stop '</s>' tags; Tokenize it into a list of lower case words\n",
    "    and bigrams\n",
    "    :param filename: string \n",
    "        filename: path to a text file consisting of lines of non-puncuated text; assume one sentence per line\n",
    "    :return: list, list\n",
    "        tokens: ordered list of words found in the file\n",
    "        bigrams: a list of ordered two-word tuples found in the file\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    bigrams = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line_tokens, line_bigrams = sentence_to_bigrams(line)\n",
    "            tokens = tokens + line_tokens\n",
    "            bigrams = bigrams + line_bigrams\n",
    "    return tokens, bigrams\n",
    "\n",
    "\n",
    "def sentence_to_bigrams(sentence):\n",
    "    \"\"\"\n",
    "    Add start '<s>' and stop '</s>' tags to the sentence and tokenize it into a list\n",
    "    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)\n",
    "    :param sentence: string\n",
    "    :return: list, list\n",
    "        sentence_tokens: ordered list of words found in the sentence\n",
    "        sentence_bigrams: a list of ordered two-word tuples found in the sentence\n",
    "    \"\"\"\n",
    "    sentence_tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
    "    sentence_bigrams = []\n",
    "    for i in range(len(sentence_tokens)-1):\n",
    "        sentence_bigrams.append((sentence_tokens[i], sentence_tokens[i+1]))\n",
    "    return sentence_tokens, sentence_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
